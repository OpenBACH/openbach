

= OpenBACH design
David PRADAS <david.pradas@toulouse.viveris.fr>
Copyright © 2016 CNES
:icons: font
:sectnums:
:sectnumlevels: 5
:toc2:
:imagesdir: images
:doctype: book
:source-highlighter: coderayz
:listing-caption: Listing
// Uncomment next line to set page size (default is Letter)
:pdf-page-size: A4

== Context of OpenBACH
The objective of this study is to design and implement a benchmark for performing tests and collect different metrologies
to allow the evaluation of next proposed transport protocols and mechanisms on a terrestrial, satellite and hybrid network.
It is important to implement an adequate measurement environment to provide an appropriate evaluation framework.

The transport benchmark would be able to quickly benefit different actors for the development of satellite network
and telecom technologies, but also the actors of terrestrial networks, which would allow to extend the number of potential users.

This benchmark must propose to integrate existing metrology tools, or exceptionally developed according to specific
needs. It must be easily operated and at the same time deliver a maximum of useful information for the understanding,
the validation and the design of existing or under development transport mechanisms. The benchmark shall also be capable
of analyzing the deployment of protocols and the options supported on clients or remote servers. In addition, different
visualizations tools and statistical analysis shall be proposed. Moreover, tools allowing to configure the network must
be implemented in order to facilitate the implementation of exhaustive testing, reproducible or even automated. Traffic
injection or traffic generation will also be deployed within the benchmark.

The test bed will be developed in an open source logic similar to the one used for OpenSAND. The components will eventually
be interfaced and / or integrate OpenSAND, in a logic that will be jointly defined by CNES, Thales Alenia Space, and Viveris
Technologies at the end of the study.


== Generic use case of OpenBACH

The objective of this project is thus to create a benchmark allowing a user to perform a set of actions presented in
<<img-generic_use_case>>. These actions are grouped in 4 different phases:
{nbsp} +
[#img-generic_use_case,reftext='Figure {counter:refnum}']
.Generic use case of OpenBACH
image::generic_use_case.png[align="center"]
{nbsp} +

*	*Phase 1*: Installation/deployment
**	In this phase, a user would wish to perform and easy and straightforward installation/deployment of any tool and/or
software that he needs in order to evaluate a network under test. That includes the installation of OpenBACH itself.

*	*Phase 2*: Preparation of scenarios and launch tests
**	In particular, the user would wish to configure all software/tool and entities under evaluation (including OS parameters,
IP, TCP stack, etc), and
**	Schedule and launch all the required tasks/software/tools for evaluating protocols, mechanisms in the network under test.
That might include traffic generators, the launch of measures acquisition.

*	*Phase 3*: Monitoring
**	The user would wish to verify that the tests are correctly running by checking the status of all entities under test, tools,
running software and/or tasks (via logs and status values), and
**	Collect the measures that have been scheduled in the previous phase and visualize real-time results.

*	*Phase 4*: Results analysis
**	Once the tests are finished, the user may wish to perform post-processing actions according to the user needs. In this phase
the user shall be able to perfom from simple actions such as shaping the obtained results (obtain averages/max/min) or more complex
 actions such as computing new results from using different inputs measures.
**	Finally, the user shall be able to visualize this new results and stock them (i.e. images, tables, etc) for creating reports
of the study that has been carried out.

OpenBACH is designed in order to be capable of fulfilling the needs of a user that wishes to perform any of the actions detailed above.



== Functional design

This sections aims at describing the overall view of the functional architecture design for OpenBACH (Benchmark Automation tools for
Communication and Hypervision).

[[section-term, Terminology]]
=== Terminology of OpenBACH

First of all, the following table allows to define some terms used in this document and in particular, those that compose
the backbone of an OpenBACH scenario:

[#tab-terminology,reftext='Table {counter:tabnum}']
.OpenBACH terminology
[frame="topbot",cols="^.^s,.^", options="header"]
|===
| Terms                         | Definition
|  Job^1^                         | A number of individual tasks (one or more) with a common purpose and to be executed in a sole Agent. A job might be able to launch/configure other software tools (e.g. ping/iperf) and OS tools (e.g. iptables), configure OS parameters, collect/generate information/stats from tools/OS, etc.
|  Job instance                 | An execution of a job configured with a set of parameters.
|  openbach-function/action     | Function defined and launched in the Controller allowing to perform tasks related to: install agents/Jobs, configure and schedule Job/scenario instances, perform information/status requests regarding Agents/Jobs/Scenarios and their instances, etc.
|  openbach-function/action instance   | An execution of an openbach-function with a set of parameters.
|  scenario                     | A set of openbach-functions that allow to perform different tasks on one or more Agents.
|  scenario instance            | An execution of a scenario with a set of parameters.
|===

^1^ [small]*A classification of Job types depending on their purpose is defined in this document.*

Other terms regarding the design of OpenBACH and used in this document are defined in the following table:

[#tab-terminology-other,reftext='Table {counter:tabnum}']
.Design terminology
[frame="topbot", cols="^.^s,.^", options="header"]
|===
|  Terms                             | Definition
|  Network Under Test               | Network under test allowing to interconnect different network entities. The real traffic (e.g. HTTP, Video streaming, etc.) is sent through this network, and it will be possibly monitored by OpenBACH
|  Management network               | Logical or physical network independent from the Network under test (or dedicated bandwidth of the physical network) allowing to interconnect each network entity with the collector and the controller of OpenBACH. This network is used to send all the signalization/messages of control, monitoring, etc., related to OpenBACH.
|  Frontend                         | It is the presentation layer and what the user is able to see, i.e. the interface between the user and the data access layer (in the backend). In summary, a mix of programming and layout that powers the visuals and interactions of the web.
|  Backend                          | It is seen as the servers-side code which has access to the data, and implements functions to manipulate this data and to use it for different purposes. In the case of OpenBACH, the backend contains the intelligence of the benchmark, i.e. the functions that allow to perform different tasks.

|===

=== Design components

OpenBACH shall implement the components Controller, Collector, Auditorium and different Agents. Their roles are detailed next:

*	A Controller shall centralize and deploy the configuration functionalities of OpenBACH as well as the jobs/scenarios to be
launched.
*	The Collector shall be able to collect all the statistics, data, logs/errors and other messages requested for supervising
the benchmark in a centralized way.
*	The Agents shall be deployed in the different end network entities (work stations, terminals, etc.), middle entities
(server, proxy, etc.) that are supposed to be controlled by OpenBACH, or even in the same entities where the Controller
and Collector are deployed. The Agents shall control (schedule/launch/stop) the jobs within a network entity according to
the Controller commands, and collect the local stats/logs sent by these jobs. As we will see, an Agent might be placed next
to the Collector and/or the Controller.
* The Auditorium component shall centralize the different frontend interfaces for configuring and monitoring (logs and
statistics) the benchmark.

A basic functional scheme of OpenBACH is represented in <<img-basic_func_design>>. From the Auditorium, a user shall be
able to configure OpenBACH and request information of it (status of entities and components). The configuration is centralized
at the Controller, which is in charge of deploying this configuration to the required Agents (the configuration might also
include the deployment of new Agents and Jobs) and asking for status information. The Agents execute/schedule/stop the Jobs
and relay the informations to be collected (statistics/logs/status) to the Collector, which centralizes all the data from
all the available Agents/Jobs. Once the information is stocked in the Collector, the Controller is able to perform requests
of data regarding the status of OpenBACH (in order to be sent to the Auditorium), and the Auditorium is able to make requests
logs and statistics in order to allow the visualization in the user PC screen.

[#img-basic_func_design,reftext='Figure {counter:refnum}']
.Design of OpenBACH interfaces
image::basic_func_design.png[align="center"]
{nbsp} +

===	Functional definitions
===	Functionality groups

OpenBACH shall propose two main functionalities: the configuration of the benchmark (including the available jobs) and the
collection of relevant data.

These two types of functionalities are well identified by color in the architecture shown in <<img-func_design>> :
•	Configuration (purple boxes/arrows): includes configuration of jobs, scenarios, entities, scheduling of jobs/scenarios.
•	Collection and display of statistics and logs/status (blue boxes/arrows) allowing to monitor the Network under Test.


[[section-func_blocks, Functional blocks per component]]
====	Functional blocks per component
Below, we list the functional blocks per component as well as the types of data flows between them that OpenBACH shall
implement. The functional architecture is shown in <<img-func_design>>.

[#img-func_design,reftext='Figure {counter:refnum}']
.Design of OpenBACH components
image::func_design.png[align="center"]
{nbsp} +

The Auditorium shall implement several frontends, one per type of display:

*	Two frontends for the configuration of OpenBACH:
**	Web interface-based: a user web interface allowing to configure and schedule the available Jobs/scenarios
of OpenBACH
**	Python scripts-based: allowing also to configure/schedule the available Jobs on each Agent from a Linux shell
terminal. The access to this frontends demands advanced users rights.

*	Two frontends (web interface-based) for displaying real-time data:

**	A dashboard frontend for log messages allowing to get, filter and show the collected logs of the benchmark.
**	A dashboard frontend for real-time statistics allowing to display the collected metrics.
*	A frontend for plotting offline and post-processed data (web interface-based).

The Controller shall implement:

*	A backend: a web server allowing to listen for user interface requests (from frontend) regarding the deployment,
 the configuration and the scheduling of OpenBACH (i.e. Agents, Jobs, Jobs instances, scenarios and scenarios instances),
as well as requests regarding OpenBACH information and status from Agents. These requests are performed using the
“openbach-functions/actions”. The Backend might be able to process itself some of the “openbach-functions/actions” regarding information
of Agents/Jobs, other “openbach-functions/sctions” need an action from the Agent side (configure/launch a Job instance, send
updated information/status, etc.). In the latter case, the Backend shall use a daemon called openbach-conductor (see below).
*	A daemon (openbach-conductor): it is in charge of taking the demands of the Backend (under the form of “openbach-functions/actions”),
and communicate them to the Agent by means of Ansible (SSH).

The Agent shall implement:

*	A Control-Agent: It shall be able to configure and execute/schedule/stop different Job instances depending on the Controller
commands (openbach-functions/actions). It also shall be able to get status/information of the Agent itself and the available Jobs, as
well as the Job instances status.
*	A Collect-Agent: it shall allow to collect statistics/data and logs from the different running Job instances of the Agent and
relay them to the Collector and locally store them. It also shall be able to send the status/information obtained by the
Control-Agent.
*	Job instances: One or several executions of a Job configured with a set of parameters. A job instance might be able to perform
different tasks and/or to collect statistics to be sent. They might be started/stopped (e.g. start/stop a ping), activated/deactivated
(e.g. iptables rules), etc. Different types of Jobs are differentiated within OpenBACH depending on the tasks that performs, such
as administration tasks or telecom/network related tasks.

The Collector shall implement:

*	A stats collector daemon: it shall centralize the data/statistics collection received from the Agents and store them into data
storages.
*	A logs collector: it shall centralize the log messages collection received from the Agents and store them into data storages.


=== Projects, scenarios, OpenBACH-functions, OpenBACH actions and Jobs (and instances)

The comprehension of these main terms is one of the keys to well understand the OpenBACH design described herein and in particular,
the way to configure the benchmark.

Besides the definition of each term (see <<tab-terminology>> at the beginning of section <<section-term>>), the purpose of this section is to explain the
relationship between this terms.

As it has been previously explained, the Jobs are the groups of tasks (under the form of scripts) that are deployed in the Agents.
An execution of this script configured with a set of parameters is known as a Job instance (an execution of the Job launched by the Agent in the
same machine). The job instances might be scheduled by openbach-functions (obfunc) when they are implemented within a scenario context, or openbach-actions when they
are independent of any scenario. Later, we will focus on the different between these two types of functions.

The openbach-functions and openbach-actions aim at performing many other tasks (other than scheduling job instances); such as the installation
of Agents, Jobs, status requests, creation of projects, etc.

==== Within a scenario context
From the controller point of view (<<img-dev_controller>>), the Controller owns the openbach-function (identified by a type and a name), different projects (identified
by his name and with one or more user owners capable of modifying the scenarios of a project), different scenarios (identified by his name, and allowing to specify different variables) and a scheduler.
A project owns one or more scenarios. A scenario owns a group of ordered openbach-functions (identified by a position id). This openbach-function might own "wait_for" elements, which are able to add
execution dependencies to the openbach-function (i.e. the openbach-function will be launched only when a specific openbach-function
instance has been launched or when a specific job instance has finished).

The scenario instance is defined as a scenario with a date and an id, and it is composed of :

* a group of scheduled openbach-functions instances, which are defined as the ordered openbach-function with all the required arguments
and a date (equal to the date of the scenario plus the offset).
* one/or more "while instances" allowing to implement the "wait_for" elements.  The while must keep sleeping while the out conditions allowing to execute
a specific (or a group of) openbach-function instance is not fulfilled.

The scheduler of the Controller is in charge of launching the scenario instance with all the openbach-function instances.

[#img-dev_controller,reftext='Figure {counter:refnum}']
.Relationship between scenario instances and openbach-functions in the Controller (and project)
image::uml_dev_controller.png[align="center"]
{nbsp} +

From the Agent point of view (<<img-agent_dev>>), the controller owns also a scheduler, the Jobs that are installed in the Agent,
defined by a job name and a description of the job. The scheduler is in charge of launching the Job instances, which are defined
as the Jobs with a date of execution, an id and the arguments. Finally, each Job instance shall be associated to a scenario instance
context represented by an id.

[#img-agent_dev,reftext='Figure {counter:refnum}']
.Organisation of Jobs and Job instances in the Agent
image::uml_agent_dev.png[align="center"]
{nbsp} +

The steps that shall be performed to schedule and launch the job instance by means of the scenario and the openbach-functions are
described below and represented in <<img-chain_sce>>.

[#img-chain_sce,reftext='Figure {counter:refnum}']
.Steps to launch a Job instance via the scenario and the openbach-functions concepts
image::chain_scenario.png[align="center"]
{nbsp} +

1.	(step 1) A user choses to launch 0 instance from the web browser (web HMI) or the shell terminal (scripts).
The request shall thus be sent via the configuration frontends (step 2). For that, the scenario shall be already created
by the user and available in the backend data base.
2.	(step 3-4) The configuration frontend shall send a request to the backend for launching the scenario instance (via HTTP).
3.	(step 5-6) The backend shall transfer to the openbach-conductor the order of launching the openbach-function to start the scenario
instance.
4. (step 7) When the scenario instance is launched, the openbach-function instances that are defined within the scenario shall be
launched/scheduled by the openbach-conductor. Some of them might imply performing tasks in the Agent, and others in the backend
(e.g. install an Agent). In fact,the openbach-functions shall specify when to launch the job instance in the Agent with respect
to a “reference starting time” of the scenario instance plus an increment delta/offset (∆)
5.	(step 8-9) (if at least one of these functions specifies to perform a task in the Agent) The openbach-conductor shall send the order
to the Agent via Ansible.
6.	(step 10-11) The Agent shall schedule the job instance when it receives the order of scheduling the job instance from
the Controller (openbach-conductor).
7.	(step 12) The launch of the Job instance is performed by the scheduler of the Control-Agent (when time = “reference
starting time” + ∆, i.e. a “reference starting time” of the scenario instance plus an increment (∆) parameter).

The “reference starting time” of a scenario instance is the time at which the scenario instance shall be launched in the backend.

==== openbach-functions and openbach-actions
Both are functions the are implemented in the openbach-conductor (in the Controller), and both functions call (point to) a "core function"
which implement the main tasks of the function (see <<img-function_action>>). The two methodologies to call the core functions are necessary in order have a direct access
from the backend and an access through the scenario:

* The openbach-action calls the core function and have an HTTP REST url/method allowing to be be accessed from the Backend. It also returns a proper
response to the backend (in HTTP REST).
* The openbach-function calls the core function but it also implements different tasks related to the scenario around the function.
They cannot be accessed from the Backend and they can only called by the same conductor within a scenario.

[#img-function_action,reftext='Figure {counter:refnum}']
.Relationship between openbach-actions and openbach-functions
image::uml_function_action.png[align="center"]
{nbsp} +

TIP: Herein and example with the core function "start_job_instance". 1. If the user wants to start a job instance independently of any scenario. The openbach-action
"start_job_instance_action" will call the core function and return a proper response to the Backend (and this to the frontend) with the "OK" status and the ID
of the job instance, or a bad request "404". 2. On the other hand, within a scenario context, the openbach-function "start_job_instance_action_of" might need to
associate the openbach-function and the scenario instance to the job instance, or it start a watch to check the status of the job instance and associate this watch
to the current scenario, etc.

Many core function have both an associated openbach-function and an openbach-action for evolutivity purposes. However, we find some exception, e.g.: all functions related
to projects (add/modify/delete) only have an openbach-action associated to the core function, since it makes no sense to modify a project in a scenario via an openbach-function.
Or for example, in the case of the openbach-functions "if_of" and "while_of", it only makes sense to use them inside a scenario (thus they cannot have an openbach-action).

IMPORTANT: This is independent for the users of OpenBACH. OpenBACH itself is able to chose between the openbach-action and the openbach-function depending on the use and the
purpose (if the request comes from the Backend (to use openbach-action) or from the conductor itself (to use openbach-function)).


====	Functional definitions of a Scenario/Scenario instance
The scenario instance is managed by the Conductor and the Backend (in the Conctroller) and centralizes the status of all the job instances received
from Agents through the Collector and the status of other openbach-functions (obfunc) (e.g. install job/agent). The states of a
scenario instance are described below (see <<img-states_sce>>, where highlighted in black we detail the states in normal conditions
and in blue those states that are used when external actions (stop/unschedule from user) or errors happen:

*	*scheduling*: when a user decides to launch a scenario, the Controller starts scheduling locally some obfunc and deploying all
the commands (via the openbach-functions)  to the different Agents.
*	*scheduled*: when the Controller receives the ok status from all Agents involved in the scenario instance and he has been able
to schedule its own obfunc. It means also that all jobs instance (job-i) have been correctly scheduled in the Agents.
*	*scheduling error*: if one of the Agents has not been able to schedule a job instance or the Controller has not been able to
schedule an obfunc, the backend will then unschedule all the job instances in all the Agents and the obfunc.
*	*unscheduling*: this state is used when the scenario instance must be unscheduled, either because a scheduling error occurred
or because once everything is scheduled (state “scheduded”), the user decides to unschedule the scenario instance.
*	*unscheduling out of control*: when at least one of the job instances or openbach-function was not correctly unscheduled (e.g.
because the agent does not respond; etc.). In that case, OpenBACH is not able to recover the control of the scenario instance and
it activates a flag “out of control” and continues with the scenario instance until it is finished (all job instances finished).
At this point, the user could manually restart the machines/agents or kill the desired job instances processes.
*	*unscheduled*: if all job instances and obfunc where successfully unscheduled. The scenario instance is considered over and suppressed.
*	*running*: a scenario instance is considered in this state when at least one of the job-i/obfunc is still running. It keeps
running while all job-i/obfunc and the Agents send an ok/running status, or if the user decides to stop it, or if the end time
is reached.
*	*running error*: when one Agent or a job-i/obfunc send an error status. If the error is considered not critical, the scenario
might keep running. If it is considered critical, the scenario instance should go to the “stopping” state.
*	*stopping*: in this state, the backend tries to stop the scenario instance (and thus all job-i/obfunc running/scheduled).
*	*stopping out of control*: similarly to the case of “unscheduling out of control”, this means that the backend was not able
to stop everything. It then activates the flag ‘out of control” and comes back to the “run” state until the scenario instance
is finished.
*	*finished*: when the end time of the scenario instance is reached with the flag out of control not activated, meaning the
scenario instance is correctly finished.
*	*finished error*: when the end time of the scenario instance if reached with the flag out of control activated, meaning
there has been a problem.

[#img-states_sce,reftext='Figure {counter:refnum}']
.Basic states diagram of scenario instance
image::basic_states_scenario.png[align="center"]
{nbsp} +

[#img-states_sce,reftext='Figure {counter:refnum}']
.States diagram of scenario instance
image::states_scenario.png[align="center"]
{nbsp} +

====	Functional definitions of a Job/Job instance
The job instance is managed by the Agent. The states of a Job instance (job-i) are described below (<<img-states_job>>):

* *scheduling*: when the order from the Controller is received, the Agent schedules the job-i.
* *scheduled*: when a job-i has been correctly scheduled in the Agent.
* *scheduling error*: if the Agent have not been able to schedule the job instance, it should send a nok to the Controller.
* *unscheduling*: this state is used when the job-i must be unscheduled because an order from the Controller is received to
do so.
* *unscheduling out of control*: when the job-i was not correctly unscheduled. In that case, OpenBACH is not able to recover
the control of the job instance and it activates a flag “out of control” and goes back to the state “running” until it is
finished. At this point, the user could manually restart the machines/agent or kill the job instances.
* *unscheduled*: if the job instance was successfully unscheduled. The job-i is considered over and suppressed.
* *execution*: when the launch time of the job-i is reached. Depending on the type of the job (persistent or not persistent),
the next state is “run” or “finished”.
* *running*: when a job is of the type persistent, it keeps running until a stop is scheduled, or if the end time is reached.
* *execution error*: when a job-i has not been correctly executed.
* *running error*: when the job-i gets an error exception. If the error is considered not critical, the job-i might keep
running. It is considered critical, the job-i should go to the “stopping” state. The way to treat the errors is carried out
by the job-I itself. Therefore, a correct treatment of the errors shall be performed when developing jobs.
* *stopping*: in this state, either the job-i tries to stop itself or it is stopped by the Agent (e.g.: if a stop order is
scheduled or reveived from the Controller)
* *stopping out of control*: similarly to the case of “unscheduling out of control”, this means that the end or the job-i
itself were not able to stop the job-i. It then activates the flag “out of control” and comes back to the “run” state until
the job-i is finished.
* *finished*: when the end time of the job-i is reached or it is stopped.

[#img-states_sce,reftext='Figure {counter:refnum}']
.Basic states diagram of job instance
image::basic_states_job.png[align="center"]
{nbsp} +

[#img-states_job,reftext='Figure {counter:refnum}']
.States diagram of a job instance
image::states_job.png[align="center"]
{nbsp} +

=====	Job types
NOTE: TBD (CNES/TAS/Viveris)

The Jobs can be classified in different types depending on its purpose, domain or even the purpose they aim at performing.

First of all, it has been highlighted the need of separation between Jobs related to administration tasks (herein called
"admin_jobs"), the Jobs related to the telecommunications domain which are the core of OpenBACH (herein called 'user_jobs").

Some examples of admin_jobs are all logs related to the modification of log level or enable/disable stats, the syncrhonization,
jobs in charge of emptying the DBs, etc.

The user_jobs shall be separated on telecommunication domain: physical, access, network, transport, service, etc. Within in each
domain, the jobs might be classified depending on the type of task the aim at performing, e.g. traffic generator, traffic monitoring,
post-processing, though some jobs might perform one or more tasks and its classification is not straightforward.


=== Material aspects: Entities

The following section describes the deployment of OpenBACH in different entities. In particular, <<img-entity_arch>> shows the architecture
and the components of the proposed design. An example of network topology where OpenBACH could be deployed is available at
the top-left corner of the figure. In such topology, the network entities are interconnected by means of heterogeneous physical
links (satellite, terrestrial, LTE, WiFi, etc.).

The scheme also shows the components of OpenBACH, the functions (and the associated functional blocks), the entities (servers,
work stations, etc.) where the components are deployed, and a management network (recommended but optional) allowing the
interaction between these components.

==== Types of entities
Five types of entities (identified as grey boxes in the figure) are defined in the <<img-entity_arch>> OpenBACH design: network
entities, user entity, controller entity, collector entity and auditorium entity.

*	A “network entity” is defined as any machine, server, or workstation, able of hosting a Linux OS (and possibly Windows OS in
further evolutions of OpenBACH) and an OpenBACH Agent component. Some examples of roles performed by these “network entities”
are: a user terminal, a server, a proxy, a gateway, a satellite terminal, a terrestrial base station.
*	A “controller entity” is defined as any machine, server, or workstation, able of hosting a Linux OS where the Controller is
deployed.
*	A “collector entity” is defined as any machine, server, or workstation, able of hosting a Linux OS where the Collector is deployed.
*	An “auditorium entity” is defined as any machine, server, or workstation, able of hosting a Linux OS where the different
frontends of the Auditorium are deployed.
*	Finally, the “user entity” is defined as any personal computer (or workstation) from which a user would be capable of supervising
and interacting with OpenBACH. This entity requires at least a shell terminal access and a web browser (Firefox or Chrome) for
accessing the OpenBACH interfaces.

For the sake of simplicity, the Collector, the Controller and the Auditorium might be deployed in the same entity.

[#img-entity_arch,reftext='Figure {counter:refnum}']
.Architecture, components and interfaces of OpenBACH
image::entity_arch.png[align="center"]
{nbsp} +

====	Functional blocks per entity


Below, we list the functional blocks, types of storage and components for each considered entity that OpenBACH shall implement:

*	A « Network entity » shall have:
**	An Agent :
***	A Control-Agent
**	A Collecting agent
***	Jobs (deployed) and Instances of Jobs (running/scheduled)
***	A path towards an available data storage: it shall allow to locally store data/logs. It is useful for offline scenarios where
the network entity is not accessible during the tests (e.g.: when a management network is not available).

*	The « Collector entity »  shall have:
**	A Collector daemon for statistics and status information.
**	A Collector daemon for log messages
**	A data base for storing logs.
**	A data base for storing statistics/data.

*	A « Controller entity » shall have:
**	A backend (web server)
**	A daemon (openbach-conductor).
**	A data Storage managed by the backend for storing information related to the benchmark (available agents and entities information,
information of jobs available, status of Jobs instances, scenarios, etc).

*	An “Auditorium entity” shall have several frontends: one per type of display (configuration of benchmark, statistics display and
logs display). In particular:
**	A frontend of configuration (web interface)
**	A python scripts interface
**	A dashboard frontend for real-time statistics dashboard (web interface)
**	A dashboard frontend for real-time log messages (web interface)
**	A frontend for plotting offline and post-processed data (web interface).


*	A « User entity » shall dispose of:
**	A web browser (Chrome/Firefox) client to access the different available frontends, i.e.:
***	Configuration web interface
***	Real-time statistics
***	Logs/errors/status
***	Post-processing or offline statistics
**	Linux/Unix shell terminals for jobs/scenarios configuration (related to the Python script frontend).


==	Detailed conception
=== Detailed conception of the Auditorium
====	Configuration frontends

Herein, we describe the design of the configuration frontends, and in particular the available supervision functions allowing to
configure OpenBACH and the different jobs/scenarios. On the other hand, the design and requirements of the other OpenBACH frontend,
i.e. those aiming at displaying the statistics/data and the log messages, are detailed in section <<section_display>> (after the
description design of the Collector and the Agents). This order is preferable since it makes the comprehension of the chosen solution
easier as well as the provided requirements of the frontends.

By means of the configuration frontends, the user shall be able to ask for different types of information regarding Agents and Jobs,
in particular, the user shall be able to ask for:

*	the list of Agents installed and their status (running/not running)
*	the list of Jobs that might be installed in an Agent (i.e. available for installation in OpenBACH). This might help a user decide
the jobs that can be installed.
*	the list of jobs available in each Agent (not necessarily running, only available)
*	the list of job instances  per Job that are scheduled/started for each Agent.
*	The scenarios available.
*	The list of scenario instances scheduled/started and their status.

This information is used by the user to have an update knowledge of the benchmark, so that he would be able to correctly perform
different tasks. The tasks that a user shall be able to carry out are:

*	Install/uninstall Agents in the network entities. The procedure for installing new Agents is explained in section <<install-agent>>
(TBD)  and in the wiki OpenBACH (http://opensand.org/support/wiki/doku.php?id=openbach:manuals:index).
*	Install/remove a job to/from an Agent
*	Schedule/start/stop a job instance in an Agent with different configuration parameters.
*	Create/delete/modify scenarios.
*	Start/stop a scenario instance over different Agents.
*	After the implementation of a new Job performed by a user, the user shall be able to make the Job available for installation.

The configuration frontend will thus serve as user interface, allowing the user to perform different tasks (as detailed above).
These tasks will be performed by calling the “openbach-actions” from the frontend in order to send the request to the core of
the Controller, also known as Backend, which will perform different actions according to the requested tasks. The benchmark shall
implement two different configuration frontends, one for basic users, which will perform different tasks through the web interface,
and a second frontend, based on python scripts, allowing for more flexibility and implemented for advanced users.

In order to maximize the evolutivity and the clarity of the backend implementation, both frontends shall be able to call/use the
same functions implemented in the backend. For this reasons, we propose a backend based on web services.

The communication between the Backend and the configuration frontends shall be carried out via an HTTP Restful API.

All the responses of the backend shall be implemented in JSON format.

=====	Web interface (Basic user)
In this section, we list some of the requirements that the frontend shall implement.

The web interface dedicated to configuration of the benchmark shall:

*	Display the status of the registered network entities (with Agents) and the collector.
*	Display the available jobs per Agent and their status.
*	Be able to configure, launch/schedule/stop the Jobs instances within a scenario.
*	Configure, display and launch/schedule/stop the available scenarios instances (by means of the openbach-actions).
*	Be able to activate/deactivate/display the available statistics.
*	Be able to activate/deactivate/display the logs (and change the log level).

===== Python scripts (Advanced users)
NOTE: To modify

===	Detailed conception of Controller

The Controller is in charge of centralizing and deploying the configuration of OpenBACH, the Agents the Jobs and scenarios and
commands the Agents to schedule the Jobs instances to be launched within a scenario instance.

As it can be observed in <<img-controller_design>> (and previously detailed, see section <<section-func_blocks>>), the controller
shall implement different functional blocks. It mainly consists of a backend for controlling the main tasks and their configuration,
a daemon (openbach-conductor) to interact with the Agents, a status-manager that manages the status of different job instances within the
scenarios and a data storage for saving information related to OpenBACH (status,
profiles, users, scenarios, etc.).

[#img-controller_design,reftext='Figure {counter:refnum}']
.Controller design: Backend and interfaces
image::controller_design.png[align="center"]
{nbsp} +

====	Backend
The backend design shall follow the Model-View-Controller (MVC) architectural pattern (as represented <<section-func_blocks>>) since
it allows a proper separation between the user-interface and the substance of the application.

In <<section-func_blocks>>, we can observe that a webserver (e.g. Apache or Nginx) shall be set up in front of the MVC pattern in
order to handle the user requests (from frontend) before passing those requests that require application logic.

The controller (of the MVC architecture) shall be in charge of receiving inputs and data from user and convert them to commands for
the views. The model shall be in charge of managing and accessing the database and the view shall contain the ways to set, compute
or manipulate information in order to send an output representation of required data.

In summary, the controller (of the MVC architecture) receives an action and data from the webserver (pushed by the user). It then
sends the data to the correct view (i.e. function), depending on the request. The view works with the model to get the appropriate
data under objects format and handles these objects in order to perform the required actions and create an output (response) to the user.

The views are the way to execute the “openbach-actions”, which are implemented in the openbach-conductor. Through these functions, the backend views shall be able to:
*	add/install (delete/remove) Agents and Jobs to/from the benchmark
*	list the available Agents and the available jobs per Agent.
*	create/modify/delete a scenario.
*	configure/launch/stop scenario instances.
*	List the available scenario and scenario instances and their status.
*	send commands of schedule/start/stop of Jobs instances to the corresponding Agents .
*	list the scheduled/started job instances and their status.

====	Ansible for communication Controller-Agent

The installation of an Agent or a Job requires the transmission of files (scripts, daemon files, configuration files, etc.), the
installation of dependencies (python, apt-get, software, etc.) and other needs such as the installation of a ntp client for
synchronizing the network entity. There are several off-the-shelf frameworks available in open-source allowing for application
deployment and/or configuration management (Puppet, Chef, Ansible, ...). The Ansible solution hasbeen retained because it is a
simple and flexible tool that gives you the ability to automate common tasks, deploy applications and launch commands in different
hosts from a centralized entity (in our case the OpenBACH Controller). In particular, Ansible implements the following features:

*	Ansible is open source and written in Python, which harmonizes with the philosophy of OpenBACH of implementing the Agent and the
Jobs in Python.
*	A scripting system based on YAML syntax, which is easily readable and with a very fast learning.
*	Everything is done via files called "playbook" (YAML syntax). The tasks written in the playbook call the Ansible modules (similar
to libraries) with different arguments (e.g. call the “apt-get” module with the option “build-dependencies” and the name of the package).
*	Ansible is only installed in the Controller. The distant hosts do not need any software requirements/dependencies to be controlled,
except for a SSH access (with the keys for authentication) and Python.
*	When playbook is executed, Ansible connects to the various entities to deploy configuration and start tasks. Thanks to the modules,
Ansible also ensures that any services that are supposed to work/run are correctly running, that a software is installed (e.g.
apt-get install packages), that a task has been performed (i.e. idempotent concept) and that all configuration files are up to date.
The last one is one of the strong points of Ansible.

====	Openbach-conductor
The Backend shall rely on a new functional item, a daemon identified as the openbach-conductor, allowing to:

* launch/manage/control complex scenario instances (over several Agent and with dependencies)
* implement a scheduler in the Controller because though the Agents control/schedule their own tasks, it is necessary also to schedule
the launch of Ansible playbooks (e.g. in case a distant network entity is only accessible at a specific time and not at the moment of
creating the test/scenario).
* avoid time out problems (associated to the webserver) when the time of execution of some playbooks are large (e.g, those installing
 Agents or dependencies, etc…). Thus the backend needs a background process (i.e. a daemon) capable of listening/controlling
 the local post-processing tasks without time constraints.


IMPORTANT: It must be highlighted that though the Controller (openbach-conductor) shall be able to process itself some of the “openbach-actions/functions”
regarding information of Agents/Jobs (stored in its data storage), most “openbach-actions/functions” need an action from the Agent side
(configure/launch a Job instances, send updated information/status, etc.).

For example, as explained latter, the Controller does not schedule the Job instances itself, instead, it commands the Agent to perform the scheduling of Jobs instances. The way the backend
communicates to the Agents is discussed next.


The openbach-conductor shall be thus in charge of listening for commands from the views, building and launching the playbooks
(via SSH to communicate the commands and the tasks to be performed in the Agents).

The commands between the views and the openbach-conductor shall be sent via UNIX sockets.

Finally, it should be highlighted that the choice of Ansible does not add limitations or constraints to the OpenBACH design since it is
developed so that any other protocol/communication would replace Ansible for deploying/configuring OpenBACH with little effort (we do not
talk here about the installation of OpenBACH and its dependencies, where Ansible probes to be an asset).

====	Status-manager

The status-manager is in charge of managing the status of all job instances launched in a scenario. When the conductor launches the openbach-function "start_job_instance",
it also tells the status-manager to monitor the status of the associated job instance that is "Running". When the status of the Job instances is "Not Running" anymore, the status-manager sends this notification
to the conductor, so that it can keep launching other tasks (if these were depending on the first job instance).

The conductor and status manager communicate by using the UNIX socket 2845 and 2846.





====	MVC
===== MVC: data access

The model shall handle one database that belongs to the backend, to save user information, agents status (running or not), a jobs list per Agent,
job instances status, scenarios (and scenario instances) information and status, etc. Some of these information are potentially continuously modified
(i.e. job instances status). For updating the status information, the Controller shall implement an openbach-action (see next section) that when
requested (or recursively) sends status information from the Agent to the Collector (via the collecting functions of OpenBACH: i.e. stats and logs). The
Controller must recursively pull these status from the Collector to update its own database.

Finally, the backend database shall implement different user profile types (see section XX).

=====	MVC: openbach-actions views
The "openbach-actions views" (kind of an access to the openbach-actions) available in the Controller are implemented in the backend, but the real
implementation of the openbach-actions is available in the openbach-conductor. These functions are summarized in <<img-functions>> and detailed below
(the input JSON contents highlighted in bold are the required ones, the other ones are optional). They are classified in 8 main groups depending on
the object/component they concern to, i.e. the Agents, the Jobs, the Job instances, the scenarios or the scenario instances.

In the tables below, we have added a column in order to show if the openbach-actions (obfunc) have also an associated openbach-function.


[#img-functions,reftext='Figure {counter:refnum}']
.Openbach-actions classified by categories
image::functions.png[align="center"]
{nbsp} +


First the group 1 of openbach-actions allowing to install, delete, list and update the status of the Collectors of the benchmark.

[#tab-ob1,reftext='Table {counter:tabnum}']
.group 1
[frame="topbot", cols="^.^s,^.^,.^e,.^,.^,.^", options="header"]
|===
| Action                      | Method      | url                         | Input contents (JSON or Query sting)                                 | Description                                                | obfunc
| add_collector                 | POST        | /collector                  | *address*, *username*, *password*, *name*, logs_port, stats_port     | Add a new Collector (and install an Agent on it)         | no
| modify_collector              | PUT         | /collector/*address*        | logs_port, stats_port                                                | Modify the Collector (and all the associated Agents)     | no
| del_collector                 | DELETE      | /collector/*address*        |                                                                      | Remove a Collector (but do not uninstall the Agent on it)| no
| get_collector                 | GET         | /collector/*address*        |                                                                      | Return the informations of this Collector                | no
| list_collectors               | GET         | /collector                  |                                                                      | Return the list of Collectors available                  | no
| state_collector               | GET         | /collector/*address*/state  |                                                                      | Return the status of the last commands on the Collector  | no
|===

Second the group 2 of openbach-actions allowing to install, delete, list and update the status of the Agents of the benchmark.

[#tab-ob2,reftext='Table {counter:tabnum}']
.group 2
[frame="topbot",options="header", cols="^.^s,^.^,.^e,.^,.^,.^"]
|===
| Action                      | Method      | url                     | Input contents (JSON or Query sting)                      | Description                                                                                                                               | obfunc
| install_agent                 | POST        | /agent                  | *address*, *username*, *password*, *collector_ip*, *name* | Install OpenBACH Agent in a network entity (identified by IP address) and add the Agent information to the Controller database.         | yes
| uninstall_agent               | DELETE      | /agent/*address*        |                                                           | Uninstall OpenBACH Agent from a network entity and delete the Agent information from the Controller database.                           | yes
| list_agents                   | GET         | /agent                  | update                                                    | Return the list of Agents, if update is present and True, this function pulls the last information status from Collector database.      | no
| retrieve_status_agents        | POST        | /agent                  | *addresses*, *action='retrieve_status'*, update           | Verify if the Controller can contact a network entity (with an Agent) and request the Agent to send its status to the Collector.        | yes
| assign_collector              | POST        | /agent/*address*        | *collector_ip*                                            | Assign this Collector to the Agent                                                                                                      | yes
| state_agent                   | GET         | /agent/*address*/state  |                                                           | Return the status of the last commands on the Agent                                                                                     | no
|===

Then group 3 of openbach-actions allowing to add/delete a Job to/from the list of available Jobs to install. The function “add_agent” might be used
if a user develops a new Job (or takes a new developed Job from someone) and includes it in the list of possible Jobs to be installed.

[#tab-ob3,reftext='Table {counter:tabnum}']
.group 3
[frame="topbot",options="header", cols="^.^s,^.^,.^e,.^,.^,.^"]
|===
| Action           | Method          | url              | Input contents (JSON or Query string)      | Description                                                                                | obfunc
| add_job            | POST            | /job             | *name*, *path*                             | Add a Job to the Jobs list (the sources are on the path and already on the Controller)   | no
| add_new_job        | POST            | /job             | *name*, *tar_file*                         | Add a Job to the Jobs list (with the sources in the tar file)                            | no
| del_job            | DELETE          | /job/*job_name*  |                                            | Delete a Job from the Jobs list                                                          | no
| list_jobs          | GET             | /job             |                                            | Return the Jobs list.                                                                    | no
| get_job_stats      | GET             | /job/*job_name*  | *type=stats*                               | Return the statistics produced by a Job.                                                 | no
| get_job_help       | GET             | /job/*job_name*  | *type=help*                                | Return the help of the Job                                                               | no
|===

Then the group 4 of openbach-actions allowing to install/uninstall a Job in a network entity (or Agent) or request/update the Job status (installed or not).

[#tab-ob4,reftext='Table {counter:tabnum}']
.group 4
[frame="topbot",options="header", cols="^.^s,^.^,.^e,.^,.^,.^"]
|===
| Action                    | Method        | url                    | Input contents (JSON or Query string)                                               | Description                                                                                                                          | obfunc
| install_jobs                | POST          | /job                   | *addresses*, *names*, *action=’install’*, severity, local_severity                  | Install one or more Jobs (identified by name) in one or more network entities (identified by IP address)                           | no
| uninstall_jobs              | POST          | /job                   | *addresses*, *names*, *action=’uninstall’*                                          | Uninstall one or more Jobs (identified by name) from one or more network entities (identified by IP address)                       | no
| retrieve_status_jobs        | POST          | /job                   | *addresses*, *action=’retrieve_status'*                                             | Request the agent to send all installed jobs to the Collector.                | no
| list_installed_jobs         | GET           | /job                   | *address*, update                                                                   | List all the installed Job for a network entity (identified by IP address). If update=False or none, the list is by default retrieved from the backend database. If update=true, this function pulls the last information status from Collector database.  | no
| set_job_log_severity        | POST          | /job/*job_name*        | *address*, *severity*, *action='log_severity'*, local_severity, date                | Set a new log severity to the Job.                                            | yes
| set_job_stat_policy         | POST          | /job/*job_name*        | *address*, *severity*, *action='stat_policy'*, stat_name, storage, broadcast, date  | Set the policy for the stats generated by this Job on an Agent (if storage=True, the Collector will store the data in the database, if broadcast=True, the Collector will broadcast the data to the Auditorium). | yes
| state_job                   | GET           | /job/*job_name*/state  | *address*                                                                           | Return the status of the commands on the Installed_Job                        | no
| push_file                   | POST          | /file                  | *file*, *path*, *agent_ip*                                                          | Push a file on the Agent.                                                     | no
| state_push_file             | GET           | /file/state            | *filename*, *path*, *agent_ip*                                                      | Return the status of the push of a file on the Agent.                         | no
|===

The group 5 of openbach-actions allowing to start/schedule/stop a Job instance in a network entity (or Agent) or request/update the Job instance status.

[#tab-ob5,reftext='Table {counter:tabnum}']
.group 5
[frame="topbot",options="header", cols="^.^s,^.^,.^e,.^,.^,.^"]
|===
| Action                    | Method        | url                       | Input contents (JSON or Query string)                                     | Description                                                       | obfunc
| start_job_instance          | POST          | /job_instance             | *agent_ip*, *job_name*, *instance_args*, *action='start'*, date, interval | Start a Job instance of the Job on the Agent.                   | yes
| stop_job_instance           | POST          | /job_instance             | *ids*, *action=’stop’*, date                                              | Stop one or more job instances using their instance id.          | yes
| restart_job_instance        | POST          | /job_instance/*id*        | *instance_args*, *action=’restart’*, date, interval                       | Stop then start an Instance. If instance_args is an empty list, the new Job instance will have the same arguments as the old one.  | yes
| watch_job_instance          | POST          | /job_instance/*id*        | *action='watch'*, date, interval, stop                                    | Request the agent to send the status of a Job instance (scheduled, running or not running) to the Collector.        | no
| list_job_instances          | GET           | /job_instance             | *address* (but can be multiple), update                                   | Return the list of the Job instances for the Agent. If update=False or none, the list is by default retrieved from the backend database. If update=true, this function pulls the last information status from Collector database.    | no
| status_job_instance         | GET           | /job_instance/*id*        | update                                                                    | Return the information of a Job Instance. If update=False or none, the status is by default retrieved from the backend database. If update=true, this function pulls the last status from Collector database.                        | no
| state_job_instance          | GET           | /job_instance/*id*/state  |                                                                           | Return the state of the commands on the Job_Instance       | no
| kill_all                    | POST          | /job_instance             | *action=kill*, date                                                       | Stop all the scenario instance, job instances and watchs.  | no
|===

The group 6 of openbach-actions allowing to create/delete/show/modify a scenario of the backend.

[#tab-ob6,reftext='Table {counter:tabnum}']
.group 6
[frame="topbot",options="header", cols="^.^s,^.^,.^e,.^,.^,.^"]
|===
| Action            | Method        | url               | Input contents (JSON or Query string) | Description                                                      | obfunc
| create_scenario     | POST          | /scenario         | *scenario_json*, project_name         | Create OpenBACH scenario.                                      | no
| del_scenario        | DELETE        | /scenario/*name*  |                                       | Delete OpenBACH scenario. The Scenario deleted is the one that is not associated to a Project, use the other route otherwise.   | no
| modify_scenario     | PUT           | /scenario/*name*  | *scenario_json*, project_name         | Replace the json of the scenario identifed by the given id.    | no
| get_scenario        | GET           | /scenario/*name*  |                                       | Return the json of the scenario identified by the given id. The Scenario getted is the one that is not associated to a Project, use the other route otherwise.    | no
| list_scenarios      | GET           | /scenario         |                                       | List all available scenarios.                                  | no
|===

The group 7 of openbach-actions allowing to start/stop a scenario instance and request for a list of scenario instance status.

[#tab-ob7,reftext='Table {counter:tabnum}']
.group 7
[frame="topbot",options="header", cols="^.^s,^.^,.^e,.^,.^,.^"]
|===
| Action                      | Method        | url                                          | Input contents (JSON or Query string)      | Description                                   | obfunc
| start_scenario_instance       | POST          | /scenario_instance                           | *scenario_name*, *arguments*, date         | Start a scenario instance. The Scenario should not be associated to a Project, use the other route for that.   | yes
| stop_scenario_instance        | POST          | /scenario_instance/*scenario_instance_id*    | date                                       | Stop a scenario instance.                   | yes
| list_scenario_instances       | GET           | /scenario_instance                           |                                            | List all the scenario instances.            | no
| get_scenario_instance         | GET           | /scenario_instance/*scenario_instance_id*    |                                            | Return the infos of the scenario instance   | no
|===

And finally, the group 8 of openbach-actions allowing to manage projects. None of the these actions are also openbach-functions.

[#tab-ob8,reftext='Table {counter:tabnum}']
.group 8
[frame="topbot",options="header", cols="^.^s,^.^,.^e,.^,.^"]
|===
| Action                      | Method        | url                                                                                       | Input contents (JSON or Query string) | Description
| add_project                   | POST          | /project                                                                                  | *project_json*                        | Add a new Project
| modify_project                | PUT           | /project/*project_name*                                                                   | *project_json*                        | Modify an existant Project
| del_project                   | DELETE        | /project/*project_name*                                                                   |                                       | Delete a Project
| get_project                   | GET           | /project/*project_name*                                                                   |                                       | Get a specific Project
| list_projects                 | GET           | /project/                                                                                 |                                       | Get all Projects
| create_scenario               | POST          | /project/*project_name*/scenario                                                          | *scenario_json*                       | Create OpenBACH scenario for this Project.
| del_scenario                  | DELETE        | /project/*project_name*/scenario/*scenario_name*                                          |                                       | Delete OpenBACH scenario of this Project.
| modify_scenario               | PUT           | /project/*project_name*/scenario/*scenario_name*                                          | *scenario_json*                       | Replace the json of the scenario identifed by the given id.
| get_scenario                  | GET           | /project/*project_name*/scenario/*scenario_name*                                          |                                       | Return the json of the scenario identified by the given id.
| list_scenarios                | GET           | /project/*project_name*/scenario                                                          |                                       | List all available scenarios for this Project.
| list_scenario_instances       | GET           | /project/*project_name*/scenario_instance                                                 |                                       | List all the scenario instances of this Project
| start_scenario_instance       | POST          | /project/*project_name*/scenario/*scenario_name*/scenario_instance                        | *args*, date                          | Start a scenario instance.
| stop_scenario_instance        | POST          | /project/*project_name*/scenario/*scenario_name*/scenario_instance/*scenario_instance_id* | date                                  | Stop a scenario instance.
| list_scenario_instances       | GET           | /project/*project_name*/scenario/*scenario_name*/scenario_instance                        |                                       | List all the scenario instances of this Scenario
| get_scenario_instance         | GET           | /project/*project_name*/scenario/*scenario_name*/scenario_instance/*scenario_instance_id* |                                       | Return the infos of the scenario instance
|===

It should be noted that a user shall be able to replay stored scenarios by simply changing the starting reference date/time (using the openbach-action
start_scenario_instance).


====	Scenario format (JSON)

The scenario backbone (in JSON) is described as follows:

[source,json,numbered]
----

{
  "name": "Ping",  # <1>
  "description": "First scenario (for test)", # <2>
  "arguments": { # <3>
     "duration": "duration of pings"
  },
  "constants": { # <4>
    "agentA": "172.20.42.167",
    "agentB": "172.20.42.90"
  },
  "openbach_functions": [ # <5>
    {
      "start_job_instance": { # <6>
        "agent_ip": "$agentA", # <7>
        "ping": { # <8>
          "destination_ip": "$agentB", # <9>
          "duration": [ # <10>
            "$duration"
          ]
        },
        "offset": 5 # <11>
      },
      "wait": { # <12>
        "time": 0, # <13>
        "launched_ids": [], # <14>
        "finished_ids": [] # <15>
      }
    },
    {
      "start_job_instance": { <16>
        "agent_ip": "$agentB",
        "ping": {
          "destination_ip": "$agentA",
          "duration": [
             "$duration"
          ]
        },
        "offset": 0
      },
      "wait": {
        "time": 10,  # <17>
        "launched_ids": [],
        "finished_ids": [0] # <18>
      }
    }
  ]
}

----
<1> "name": the name of the scenario
<2> "description": a description of the scenario.
<3> "arguments": a list of arguments. An argument owns a name and a description.
<4> "constants": a list of constants. A constant owns a name and a value.
<5> "openbach_functions": a list of openbach-functions.
<6> the name of the openbach-function ("start_job_instance"). Each openbach-function has different elements (see later)
<11> "offset": the openbach-function will be launched a time "offset" after the beginning of the scenario.
<12> "wait": a structure used by the conductor to wait for a specific action (see below) before launching the current openbach-function.
<13> "time": the time that the conductor waits before launching the current openbach-function if the conditions below are fulfilled
<14> "launched_indexes": the id of the openbach-function that should be already "Finished"" before launching the current openbach-function
<15> "finished_indexes": the id of the job instance that should "Not running" anymore before launching the current openbach-function

The elements of the openbach-function "start_job_instance" are:

<7> the agent where the job instance should be scheduled/launched
<8> the name of the job with the arguments of the job (destination ip <9> and duration <10>)


IMPORTANT: The arguments and constants of the scenario can be used also by the openbach-functions by using the "$" followed by name of the arguments/constant (as in <7> and <9>).
Thus a user could make the scenario arguments dynamic without modifying the scenario itself (only the arguments).

TIP: Example of dependency in the scenario: the second openbach-function <16>, will be launched 10 seconds <17> after the first openbach function with id "0" <18>.

===== Status of the instances for Scenarios/openbach-functions and jobs

The available status of the instances in the current version of OpenBACH are (not all status shown in <<img-states_sce>> and <<img-states_job>> are already available):


.Scenario instances status
[width=30%, grid="none", frame="topbot", options="header", cols="^.^"]
|===
| Status
| Scheduling
| Running
| Finished OK
| Finished KO
| Stopped
|===


.Openbach-function instances status
[width=30%, grid="none", frame="topbot", options="header", cols="^.^"]
|===
| Status
| Scheduled
| Running
| Finished
| Stopped
| Error
|===

.Job instances status
[width=30%, grid="none", frame="topbot", options="header", cols="^.^"]
|===
| Status
| Scheduled
| Running
| Not Running
| Error
|===


=====	Validity of a scenario
In order to check if a scenario is 'Finished OK', the conductor needs to verify that all the openbach-functions instances and the job instances


NOTE: TODO

=====	If and while

NOTE: TODO


====	Justification of Djando framework
Django is an open-source Python web development framework.  First of all, it has been chosen since it is implemented in Python, which allows to harmonize
with the philosophy of OpenBACH (the Agent and the Jobs are developed in Python). Among the available Python frameworks, Django is known for offering
off-the-shelf functionalities (data access methods, optimized database structures, plugins for interfacing with different applications, profiles management,
etc.) allowing to focus on the pure development and the core functionalities required for the backend of OpenBACH.

Django is defined by their creator as a framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes
care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel.

===	Detailed conception of Collector

As it has been previously presented in the overall design of OpenBACH, the Collector component shall be in charge of centralizing the collection of two main
groups of data: the statistics/data and the logs.

The Collector shall be able to receive and collect two types of stream messages: logs and stats/metrics. Each type of stream shall implement its own daemon
and its own database. The way OpenBACH collects the two types of data has been properly distinguished within the chain of functional blocks of <<img-gen_collector>>.

Both collections shall have the same functional scheme: a pure collector represented by a daemon that listens for new messages sent by the Agents, and a
proper data base with efficient search mechanisms an access features, where the daemon stores the statistics and logs.

The fact of differentiating between two different streams (and databases), one for logs and another one for stats, is necessary since the nature and the
format of each one is very different. For example, logs need a database capable of efficiently indexing and filtering long messages depending on host/job/type/etc,
while stats need a high precision when time stamping and storing the data.

[#img-gen_collector,reftext='Figure {counter:refnum}']
.Generic functions of the Collector and interfaces
image::generic_collector.png[align="center"]
{nbsp} +

Regarding the interfaces of communications: the Collector daemon shall listen on a UDP/TCP socket, where all the Agents transmit their respective messages.
The daemon shall store the data into a local data base via an HTTP API. Any external access to the data base (e.g. visualize the data in a web interfaces)
shall be performed by means of this HTTP API.

The data received can be flagged. The flag can precise if the data should:

* be stored in the database
* and/or broadcasted to the Auditorium. The broadcast is done on an TCP or UDP socket (configurable) on the port 2223.

[#tab-flags,reftext='Table {counter:tabnum}']
.Flag of stats
[width=60%, grid="none", frame="topbot", options="header", cols="^.^, ^.^, ^.^"]
|===
| Stored in DB | Broadcasted	     | Flag Value
|  no          | no                | 0
|  yes         | no                | 1
|  no          | yes               | 2
|  yes         | yes               | 3
|===

As detailed in the following two sections, off-the-shelf open-source software solutions have been chosen for fulfilling the needs of OpenBACH, and in order to
have a robust collecting system at the disposal of OpenBACH. Moreover, this choice allows to focus more effort on the design and the development of an evolutive
and robust configuration/control function (one of the critical points of this benchmark).

====	Logs collection details
Concerning the logs, the collector daemon function is performed by Logstash and the database role is carried out by Elasticsearch.

Logstash is an open-source data collection (under Apache 2 license), and a data transportation pipeline. It allows to efficiently process a growing list of logs, events and unstructured data sources for distribution into a variety of outputs, including the one used herein, an Elasticsearch data base. It is capable of normalizing different data formats by means filters.

Thus, once Logstash collects a log, it sends it to ElasticSearch, a database developed by the same creators of Logstash. The main features of Elasticsearch are:
*	It has an indexing engine allowing fast search of data.
*	Real-time analytics of the stored data
*	It is API driven by a simple Restful API using JSON over HTTP. Log search is performed by this means.
*	The requests/queries are returned in common text formats like JSON.
*	It is available under Apache 2 open-source license.


Below, it is shown an example of the way logs can be exported from ElasticSearch via the HTTP API (check Elasticsearch manuals for more information). In the example, two filters are used for:
*	exporting the logs within a 10 seconds time range, and
*	returning only log-type-one logs lines

[source,json]
----
curl -XGET http://localhost:9200/playground/equipment/1?pretty
{
"_source": "message",
"filter": {"type": {"value":"log-type-one"}},
"query": {"range": {"@timestamp" : { "gte":"2015-02-20T12:02:00.632Z", "lt": "2015-02-20T12:02:00.632Z||+10s"}}}
}
----

====	Statistics collection details

In the case of the statistics collection, we take profit of InfluxDB as a database, an open-source platform for data collection and storage. We use Logstash here too as the collecting daemon. Logstash is capable of listening on a UDP/TCP socket from the Agents messages (on the port 2222), and redirects the collected data to InfluxDB using an HTTP API. Otherwise, the Agent would have had to insert the data directly into the database (via HTTP), which would have made the Agents dependant on the type of database.

InfluxDB is capable of handling data time series with high precision (1ms if necessary) when the constraints of performance and availability are strong.

The external access to the InfluxDB data storage is also realized by means of this HTTP API. InfluxDB comes with a web HMI allowing to visualize or add raw data for advanced users.

Below, it is shown an example of writing and querying formats to be used when interacting with InfluxDB database via the HTTP API (check InfluxDB manuals for more information):

*	Writing data: a POST shall be sent to the database (e.g. name mydb). The data consists of the measurement “cpu_load_short”, the tag keys host and region with the tag values “server01” and “us-west”, the field key value with a field value of “0.64”, and the Unix Timestamp  “1434055562000000000”.
[source,json]
----
curl -i -XPOST 'http://localhost:8086/write?db=mydb' --data-binary 'cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000'
----

* Querying data: to perform a query, a GET request shall be sent. It shall set the URL parameter “db” as the target database, and set the URL parameter “q” as your query. The example allows to query the same data was written in the POST example.
[source,json]
----
curl -G 'http://localhost:8086/query?pretty=true' --data-urlencode "db=mydb" --data-urlencode "q=SELECT value FROM cpu_load_short WHERE region='us-west'"
----

InfluxDB is released under the open-source MIT License.


===	Detailed conception of Agent

The Agent component shall implement two main parts according to the main functionalities of OpenBACH, a Control-Agent for configuring and controlling
the Agent, and the Collect-Agent for everything related to statistics and logs collection. These two main parts are represented in <<img-agent_design>>. as the
two grey boxes.

A reliable communication protocol shall be used to receive the commands and configuration from the Controller. As it has been previously
explained, the chosen protocol is SSH. Represented as a red box in <<img-agent_design>>, we can observe the virtual SSH connection created and managed by
Ansible (from the Controller). This methodology shall be also used to modify the configuration of log severity level and the activation/deactivation
of statistics.

The Control-Agent shall be in charge of scheduling, executing, checking and stopping the Jobs instances (green box) available in the network entity.
As it has been previously defined, a job can be defined as a number of individual tasks, i.e. start a traffic generator, start collecting a new
data/statistics, start a service, etc.

[#img-agent_design,reftext='Figure {counter:refnum}']
.Detailed design of OpenBACH Agent including its interfaces
image::agent_design.png[align="center"]
{nbsp} +

====	The Control-Agent part

The Control-Agent shall implement:
* A daemon for centralizing the tasks/jobs control (“openbach-agent” in <<img-agent_design>>),
* a generic small bash script (“openbach-baton” in <<img-agent_design>>) that the Controller uses to communicate with the daemon, and
* a scheduler (integrated in the daemon “openbach-agent” and based on the Python library “apscheduler”) for launching/scheduling the tasks of the daemon.

The communication shall be performed as follows:

* Step 1 (already seen in the Controller design): Depending on the “openbach-function/action” called in the Controller, the openbach-conductor (in the Controller)
builds a playbook and creates an SSH connection with the Agent by means of Ansible. The playbook consist of a simple command allowing to execute the
“openbach-baton” with a set of parameters (see an example at the end of this section XXX).

* Step 2: The playbook executes the ”openbach-baton” script with the set of required parameters.

* Step 3: “openbach-baton” forwards the order to the daemon (“openbach-agent”) by means of a local socket.

* Step 4: The daemon “openbach-agent” registers the received command and executes its tasks/actions (known as “agent-actions”). These actions are related to the
“openbach-functions/actions” implemented in the Controller as it is detailed next.

TIP: Justification for the need of “openbach-baton”: The shell script “openbach-baton” is only a tool to transmit the order/command from the Controller
to the Agent. It is needed because Ansible, which is the mean of communication between the two components, needs not only a way to relay the commands to the
Agent but also to get a proper response from the Agent (i.e. a response that Ansible would be able to interpret, e.g. OK status). Moreover, the use of
“openbach-baton” would allow to replace the communication mechanisms if wanted (for example, introducing a communication between web services for sending
some commands instead of Ansible.)

The use of bash for this relay seems a reliable way of interfacing the entities.

Below, the main features of the Agent are described:

* The Agent shall be based on a request-to-do policy, i.e. it shall perform tasks only when the Controller asks for.
* Within the command, the Agent may receive start/stop date-time information from the Controller, so that it will know when to execute the “agent-function”
associated to.
* Depending on the command type, other options can be used as described below.
* The Agent shall manage the scheduler locally, so that it will be able to control the whole execution/status of the agent-actions.
* The Agent scheduler shall be able to execute the agent-actions with one millisecond accuracy.

=====	Agent actions
The agent-actions are a group of actions implemented in the "openbach-agent" that allow performing different tasks regarding the OpenBACH control. These actions
are directly related to the “openbach-functions/actions”, since as it has been explained, in some cases, these functions need to perform actions/tasks in the Agents
side (groups 3 and 4 shown in <<img-functions>>), and the agent-actions are their way to do it.

<<tab-agent_func>> shows the mapping between the openbach-function/action of the Controller, the openbach-baton commands (commands sent via Ansible) and the agent actions
implemented in the Agent.

[#tab-agent_func,reftext='Table {counter:tabnum}']
.Mapping between "openbach-functions/actions" (implemented in the Controller) and the agent-actions (implemented in the openbach-agent)
[frame="topbot",options="header", cols="^.^, .^, .^, .^"]
|===
| Openbach-function / action (in Controller) and group   | openbach-baton commands          | agent-action/s (in Agent)                                       | Objective of agent-action
| install_job / 3                | add_job_agent + args              | add_job_agent                                                                 | To inform the agent about a new installed job.
| uninstall_job / 3              | del_job_agent + args              | del_job_agent                                                                 | To inform the agent about an uninstalled job.
| retrieve_status_jobs / 3       | status_jobs_agent                 | status_jobs_agent                                                             | To send the status of installed jobs to the Collector.
| start_job_instance / 4         | start_job_instance_agent + args   | start_job_instance [small]*(calls schedule_job_instance(myfunc="launch_job"))*           | To start a job instance in the Agent
| stop_job_instance / 4          | stop_job_instance_agent + args    | stop_job_instance [small]*(calls schedule_job_instance_stop(myfunc="stop_job"))*         | To stop a job instance in the Agent
| restart_job_instance / 4       | restart_job_instance_agent + args | {stop+start}_job _instance                                                    | To restart a job instance in the Agent
.2+| status_job_instance / 4   .2+| status_job_instance_agent + args | schedule_watch [small]*(calls status_job when type "date"/"interval")*                | To send the status of a job instance (scheduled, running, not running ...) to the Collector. | shedule_watch [small]*(calls stop_watch when type "stop")* | To stop sending the status of a job instance to the Collector.
|===

Next, it is detailed the different commands that the Control-Agent shall accept from the Controller component:

[source,python,small]
----
'status_jobs_agent': (0, ''),
'add_job_agent': (1, 'You should provide the job name'),
'del_job_agent': (1, 'You should provide the job name'),
'status_job_instance_agent': (4, 'You should provide a job name, an '
                'instance id, a watch type and its value'),
'start_job_instance_agent': (6, 'You should provide a job name, an '
                'owner_scenario_instance_id, a scenario_instance_id, a '
                'job_instance_id, the type of start (date or interval) and its value. '
                ''Optional arguments may follow (arguments of the Job)',
'restart_job_instance_agent': (6, 'You should provide a job name, an '
                'owner_scenario_instance_id, a scenario_instance_id, a '
                'job_instance_id, the type of start (date or interval) and its value. '
                'Optional arguments may follow (arguments of the Job)',
'stop_job_instance_agent': (4, 'You should provide a job name, an '
                'instance id, the type of stop (date) and its value'),

----

Below, we show an example of playbook task allowing the Controller to execute the “openbach-baton” script, which then transmits the command “start job”
to the Agent:

[source,json]
----
- name: Start Job instance
  shell: /opt/openbach-agent/openbach-baton start_job_instance_agent {{ job_name }} {{ job_instance_id }} {{ scenario_instance_id }} {{ owner_scenario_instance_id }} {{ date_interval }} {{ job_options }}
----

A configuration file for each job shall be implemented. This configuration file shall be used for verification purposes
(e.g. check arguments/parameters/options accepted by the job) and making a job persistent (once it has been installed). The configuration file
format shall include 4 sections (general information, the os requirements, the accepted arguments and the to be produced statistics):

[source,json]
----
---
general:
  name:            fping
  description: >
      This Job executes the fping ...
  job_version:     0.1
  keywords:        [ping, fping, rate, rtt, round, trip, time]
  persistent:      true # <1>

os:
  linux:
    requirements:  'Ubuntu 14.04/16.04'
    command:       '/opt/openbach-jobs/fping/fping.py'  # <2>
    command_stop:

  windows:
    requirements:  'Windows 2010'
    command:       '...'
    command_stop:

arguments:  # <3>
  required:
    - name:        destination_ip
      type:        'ip'
      count:       1
      description: >
          The destination ip of the fping
  optional:
    - name:        count
      type:        'int'
      count:       1
      flag:        '-c'
      description: >
          Stop after sending count ECHO_REQUEST packets. Default is 3.
    - name:        interval
      type:        'int'
      count:       1
      flag:        '-i'
      description: >
          Wait interval seconds between sending each packet.

statistics:  # <4>
    - name:        rtt
      description: >
          The Round trip time of ICMP packets.
      frequency:   'every *count x interval* sent packets or every *duration* time'

----
<1> The persistent variable should be a Boolean. It indicates if the job shall run on background or if it will only execute some tasks and finish.
<2> Command to be executed by the “openbach-agent” daemon on the agent when starting the job instance. (i.e. the path to the job script)
<3> Accepted "required" and "optional" arguments
<4> Produced statistics

When the Agent crashes or if it is restarted, the job configuration files help the Agent to know its own jobs before crashing/restarting.

Finally, it should be highlighted that the way the Agent has been designed would allow a user to control each Agent without a Controller, in other words, the
current design would allow to bypass the Controller component if an advanced user needs to do so (see the debug section in OpenBACH wiki for more information ).

====	The Collect-Agent part

The Collect-Agent shall implement two different client for collecting statistics and logs. The collection and forward of logs shall be performed by Rsyslog
(open-source tool) and the collection and forward of stats/metrics shall be performed by the rstats client.

NOTE: Rstats is a home-made program that collects stats and sends them to the Collector. Its principle is similar to the one of statsd (a simple daemon for stats aggregation)
but modified in order to fulfill the OpenBACH requirements (in terms of accuracy, performance, etc.)

Two jobs (admin_jobs) shall be dedicated to control the collecting daemons (as shown in the figure): the Job “rsyslog” and Job “rstats” which shall allow to start/stop/restart/reload
the rsyslog and rstats daemons, as shown later.


Regarding the logs, data and statistics to be collected, the Job instances shall be in charge of sending the logs/stats to the two daemons of the Collect-Agent (i.e.
Rstats and Rsyslog). For that, the "collect agent API" shall be imported in the jobs script to be able use different methods (register_collect, send_log, send_stat, reload_stat, remove_stat, ...)
allowing to send the stats from the Job instance to the Collect-Agent daemons, which will forward the stats/logs to the Collector component via UDP/TCP sockets.

TIP: The collect-agent API allows to transparently treat logs and stats (independently of the clients rsyslog/rstats)

==== Rsyslog

Rsyslog shall be used in the Agents to handle the logs of the different running Jobs. It shall then forward the log messages to the Collector via a UDP/TCP socket.
The configuration parameters to be used for rsyslog per Job shall be:

* Collector IP Address
* Logstash port:10514 (default port)
* Local log severity level (to locally store in the network entity)
* Remote log severity level (to send to the collector)
* Job Name
* Scenario ID and job instance ID

Thus the Controller (after a user request) can specify the severity level that the Agents will use for both sending the logs to the Collector and locally store them in the network
entity. The way these parameters are modified is explained at the end of this section.



The log messages (string format), shall be handled by a Python “Rsyslog API”. The number and types of severity levels are chosen among those ones defined for Syslog standard messages,
it is proposed to use the following ones:

[#tab-flags,reftext='Table {counter:tabnum}']
.OpenBACH Log level
[frame="topbot", width="60%", options="header", cols="^.^,^.^, .^"]
|===
| Value       | Severity	               | Keyword
|  0          | Error                    | syslog.LOG_ERR
|  1          | Warning                  | syslog.LOG_WARNING
|  2          | Informational            | syslog.LOG_INFO
|  3          | Debug                    | syslog.LOG_DEBUG
|===

==== Rstats

Rstats has the same role as Rsyslog but focused on statistics collection and relay. Rstats shall fulfill the following requirements:

* Aggregate the statistics/metrics sent from the available jobs.
* Time stamp each collected statistics with one millisecond accuracy.
* Relay the statistics to the Collector, and allow to activate/deactivate this option for each statistic.
* Add a flag to the data, so the collector knows if it has to store and/or broadcast the received data
* Locally store all statistics.

The flag can be :

* 0 for no storage and no broadcast
* 1 for storage and no broadcast
* 2 for no storage and broadcast
* 3 for storage and broadcast

If the flag is 0, Rstats only stores locally the statistics and does not send the data to the collector.

The deactivation/activation of a statistic shall be realized by means of the following configuration file (one configuration file per statistic):

[source,conf]
----
[default]
storage=true
broadcast=false
----

For example, in this configuration file, the statistics are send to the collector with a flag 1. The collector only stores the statistics in InfluxDB.

Rstats communicates with the Collector on an TCP or UDP socket on the port 2222.


==== Collect-agent API / How to use

Herein, we show an example on how to use the collect-agent API in a Job script:

[source,python]
----

import collect_agent    # <1>
conffile = "/opt/openbach-jobs/job_name/job_name_rstats_filter.conf"
success = collect_agent.register_collect(conffile)    # <2>

collect_agent.send_log(syslog.LOG_ERR, "ERROR: %s" % exception)  # <3>

statistics = {'rtt': rtt_data}
collect_agent.send_stat(timestamp, **statistics) # <4>

----
<1> import the API
<2> register the job instance to collect_agent
<3> send a log
<4> send a stat of type "rtt" and value "rtt_data" with a timestamp

===== Log severity level and activation/deactivation of stats
This section aims at detailing the way the OpenBACH Agent modifies the loglevel severity and activates/deactivates the stats:

- Step 1: After a user request asking for a new modification, the controller sends (using an Ansible playbook and a SSH connection) a new configuration file (for the aimed Job/Jobs)
- and the command allowing to reload the job “Rstats” (for stat activation/deactivation) or restart the job ‘Rsyslog” (for a log severity level modification).
- Step 2: The file is stored in the directory used by Rsyslog and/or Rstats clients.
- Step 3: The openbach-agent executes the command for the concerned Job (Rstats or Rsyslog)
- Step 4: Rsyslog and Rstats clients are restarted/reloaded in order to take the new configuration for logs and statistics.

===	Interfaces

The interfaces between all the components, the databases, the HMI, and the different blocks (representing different functionalities) is one of the keys to design
reliable and robust communications protocols/APIs between all of them.

As it can be seen in <<img-interf_design>>, where the main interfaces are displayed and listed, HTTP shall be used for communication between most of the elements,
mainly in the case of user-to-frontend interfaces, or for frontend-to-backend interfaces (e.g. for web services), as it is a mature technology and it is very well c
considered among the community. Even the access to the different databases (InfluxDB and ElasticSearch) shall be carried out by means of HTTP API, which allows easy
data portability, and fast query/request of data, etc.

Sockets (well known by his efficiency and simplicity of implementation) shall be mainly used for log/statistics transmission between Agent and Collector daemons,
and between Jobs instances and Agents. For example, the Agent obtains the logs/statistics from jobs instances by means of local UNIX sockets, and transmits them to
the collector by means of UDP/TCP sockets.

For local communication, where no data is transmitted, the elements shall communicate with simple bash/script commands (i.e. for execute/launch a task/process).
That is the case of, for example, the Agent (controlling part)-to-job interface, or the Agent (controlling part)-to-(collecting part) interface.

Finally, the communication between the Controller (mainly the Backend) and the Agents shall be performed via SSH/SFTP communications. The SSH/SFTP communication
is managed by Ansible. Further details about Ansible are given in section .

[#img-interf_design,reftext='Figure {counter:refnum}']
.Basic overall design of OpenBACH components
image::interfaces_design.png[align="center"]
{nbsp} +

==== Detailed Controller-Collector-Auditorium interfaces
An overall architecture of the auditorium,  the controller and the collector and their interfaces is shown in <<img-controller_collector>>, where we can observe that the main streams
of information between these components are those related to the writing/querying of data to/from the Collector databases (both logs DB and stats DB).

Indeed, once the Collector stores the logs and statistics in their databases, the Controller and the Auditorium shall be capable of pulling this data for visualization
and post-processing. Therefore:

* The frontends (both for logs ans statistics, described in Section 8) for displaying the logs and statistics shall use an HTTP API provided by the stats/logs databases
 for getting the data to be displayed.
* The Controller backend shall be able to query information stored in the database regarding job instances status (scheduled/started/finished) by means of a proposed HTTP
API.

[#img-controller_collector,reftext='Figure {counter:refnum}']
.Auditorium, Controller and collector interfaces design
image::controller_collector.png[align="center"]
{nbsp} +


===	Post-processing

====	Import/export

As it has been detailed in the OpenBACH design, the Jobs are the way to execute the post-processing tasks allowing to perform dedicated calculations of the collected statistics.

A variant of the functional scheme of OpenBACH that is used for performing operations on the collected data via the post-processing jobs is shown in <<img-design_post_processing>> (highlighted
in red), where:

* After a user choses to launch a post-processing job (the same way any other Job is launched)
* The Job instance shall pull the required data from the statistics/logs database (InfluxDB and/or ElasticSearch) of the Collector (via the HTTP API). Then it shall perform the calculations
and push the new data the same way a Job instance sends data to the Collector (i.e. via collect-agent: rstats). In that case, the Job (script) shall contain a module to access the database.


The module "CollectorConnection" ("import CollectorConnection from data_access") has been implemented in order to be capable of exporting data from InfluxDB and ElasticSearch. It contains different functions allowing to access and export
data from the Collector. See <<img-data_access>> for a detailed view of the module functions.

NOTE: The CollectoConnection module (in data_access) is not available in the repository of openbach/openbach (main project of OpenBACH) but in openbach/openbach-api. You should obtain the sources from "git@forge.net4sat.org:openbach/openbach-api.git"

[#img-design_post_processing,reftext='Figure {counter:refnum}']
.Post-processing pull (import)/push(export)
image::design_post_processing.png[align="center"]
{nbsp} +


[#img-data_access,reftext='Figure {counter:refnum}']
.Class Diagram of the data access (export of data) for logs and stats
image::data_access.png[align="center"]
{nbsp} +

Two constraints must be taken into account for correctly pushing the post-processed data into the Collector:

* The post-processed data and the original data shall not have the same name, in order to avoid deleting the original data.
* The post-processed data shall be time stamped: sometimes with the same time stamp of the original data, but it could also be time stamped with a different time (depending on the user needs).

IMPORTANT: the developer of the post-processing job must take into account these constraints.


====	Post-processing jobs
The benchmark shall include post-processing jobs allowing to compute the variance, the CDF, the interval of confidence and the average values over a time window.

These jobs shall allow to extract data from the InfluxDB database, compute the required post-processed values and export them into InfluxDB.


===	Display of data collection

The objective of this section is to first remind the full data collection chain of OpenBACH, including the collection carried out by the jobs and the centralization
of the data in the collector. Secondly, we aim at showing the details regarding the considered display options for the different types of data collected in OpenBACH.
We aim at presenting the requirements and the design regarding the data display frontends (real-time data, real-time logs and offline data)

==== Real-time logs
As it has been detailed previously, the Collect-Agent daemon (Rsyslog) is in charge of collecting the different logs sent by the running job instances in a network
entity. Those logs are then relayed to the Collector (via UDP/TCP sockets), which stores them into the chosen Logs database (i.e. Elasticsearch).

Regarding the logs display, OpenBACH shall offer a web interface (via Firefox/Google Chrome web browsers) for visualizing the collected logs on real-time.

The Log messages displayed shall at least contain the following information:

* Time/date of log message collection
* Log level
* ID of the network entity (e.g. hostname)
* Name of the Job sending the log message
* Scenario ID and job instance ID (if they are generated by a job instance)
* The message

Moreover, the logs web interface shall propose tools allowing to perform:

* Logs research
* Logs filtering (e.g. filters for host machine, IP, job, log level, etc.)
* Different auto refresh intervals, from 5 seconds to several hours.
* Calculation of number of statistics per applied filter, per time window.

Kibana has been chosen as frontend for the logs web interface. It is an open-source data visualization platform that allows a user to interact with the collected data, organize
and plot different graphics and create your own logs dashboards. It is able to use the HTTP Restful API to query logs from Elasticsearch.

[#img-collect_logs,reftext='Figure {counter:refnum}']
.Collection and display of log messages.
image::collecting_logs.png[align="center"]
{nbsp} +

==== Real-time statistics

Concerning the collection of statistics, the Collect-Agent daemon (Rstats) is in charge of collecting the different stats sent by the running jobs instances in a network entity.
Those stats are then relayed to the Collector (via UDP/TCP sockets), which stores them into the chosen Stats database (i.e. InfluxDB).

Regarding the stats display, OpenBACH shall offer a web interface (via Firefox/Google Chrome web browsers) for visualizing the collected stats on real-time.

The statistics name shown in the web interface shall be able to be chosen depending on:

* The statistic name (and Job name)
* The ID of the network entity (e.g. hostname)
* The time/date of data sample
* Scenario ID and job instance ID
* The data

Moreover, the stats web interface shall propose tools allowing to perform:

* Statistics research per host and per job instance.
* Simple calculation such as maximum/minimum/average values.
* Different auto refresh intervals, from 5 seconds to several hours.
* Snapshot of the graphics (in order to share them or use them in documents).

Grafana has been chosen as frontend for the stats web interface. It is an open-source dashboard for data display that allows a user to visualize and interact with
the collected data, organize and plot different types of graphics and create your own dashboards. It is able to use the HTTP API to query the statistics from InfluxDB data base.

[#img-collect_stats,reftext='Figure {counter:refnum}']
.Collection and display of real-time statistics.
image::collecting_stats.png[align="center"]
{nbsp} +


==== Post-processing data statistics

Regarding the offline display, OpenBACH shall offer a web interface (via Firefox/Google Chrome web browsers) for visualizing the post-processed metrics and other offline statisics.

The offline web interface shall propose tools allowing to perform:

* Advanced manipulation of graphics
* Snapshot of the graphics (in order to share them or use them in documents).
